# ğŸ¤– Disease Classifier

Machine learning classification models for diabetes prediction using preprocessed clinical data.

## ğŸ¯ Project Overview

This project implements multiple machine learning classification algorithms to predict diabetes outcomes based on clinical biomarkers. The project uses the cleaned dataset from the Health Data Analyzer and trains four different models for comparison.

**Objectives:**
- âœ… Split data into training and testing sets
- âœ… Train multiple classification models
- âœ… Compare model performance
- âœ… Save trained models for deployment

## ğŸ“Š Dataset

**Source:** Preprocessed data from `01-health-data-analyzer`
- **Training samples:** 614 (80%)
- **Testing samples:** 154 (20%)
- **Features:** 8 clinical measurements
- **Target:** Binary diabetes outcome (0=negative, 1=positive)
- **Class distribution:** ~65% healthy, ~35% diabetes (stratified split)

### Input Features

| Feature | Description | Unit |
|---------|-------------|------|
| Pregnancies | Number of pregnancies | count |
| Glucose | Plasma glucose concentration | mg/dL |
| BloodPressure | Diastolic blood pressure | mm Hg |
| SkinThickness | Triceps skin fold thickness | mm |
| Insulin | 2-Hour serum insulin | mu U/ml |
| BMI | Body mass index | kg/mÂ² |
| DiabetesPedigreeFunction | Diabetes heredity score | score |
| Age | Age | years |

## ğŸ“ Project Structure

```
02-disease-classifier/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ diabetes_cleaned.csv    # Preprocessed dataset (from project 01)
â”‚   â”œâ”€â”€ train_data.csv          # Training set (614 samples)
â”‚   â””â”€â”€ test_data.csv           # Testing set (154 samples)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 01-split_data.py        # Train/test split
â”‚   â””â”€â”€ 02-train_models.py      # Model training
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logistic_regression.pkl  # Trained LR model
â”‚   â”œâ”€â”€ decision_tree.pkl        # Trained DT model
â”‚   â”œâ”€â”€ random_forest.pkl        # Trained RF model
â”‚   â””â”€â”€ knn.pkl                  # Trained KNN model
â”œâ”€â”€ results/
â”‚   â””â”€â”€ training_results.csv     # Training metrics summary
â””â”€â”€ notebooks/
    â””â”€â”€ evaluation.ipynb         # Model evaluation (planned)
```

## ğŸš€ Setup & Usage

### Prerequisites
```bash
Python 3.8+
pip or conda
```

### Installation
```bash
# Navigate to project directory
cd PROJECTS/02-disease-classifier

# Install dependencies
pip install -r requirements.txt
```

### Running the Pipeline

#### 1ï¸âƒ£ Split Data into Train/Test Sets
```bash
python src/01-split_data.py
```
**Output:** 
- `data/train_data.csv` (614 samples)
- `data/test_data.csv` (154 samples)

**Details:**
- 80/20 train-test split
- Stratified sampling (maintains class distribution)
- Random state = 42 (reproducible)

---

#### 2ï¸âƒ£ Train All Models
```bash
python src/02-train_models.py
```
**Output:**
- 4 trained models saved in `models/`
- Training metrics in `results/training_results.csv`

**Models Trained:**
1. **Logistic Regression** - Fast, interpretable baseline
2. **Decision Tree** - Rule-based, easy to visualize
3. **Random Forest** - Ensemble method, high accuracy
4. **K-Nearest Neighbors** - Instance-based learning

---

## ğŸ¤– Models & Results

### Training Performance

| Model | Training Accuracy | Training Time |
|-------|------------------|---------------|
| Logistic Regression | 77.69% | ~0.01s |
| Decision Tree | 84.69% | ~0.01s |
| Random Forest | 81.92% | ~0.15s |
| K-Nearest Neighbors | 82.74% | ~0.00s |

> **Note:** These are training accuracies. Test performance evaluation is planned.

---

## ğŸ“ˆ Model Details

### 1. Logistic Regression
**Type:** Linear classifier  
**Pros:** Fast, interpretable, good baseline  
**Cons:** Assumes linear relationships  
**Hyperparameters:** `max_iter=1000`, `random_state=42`

---

### 2. Decision Tree
**Type:** Tree-based classifier  
**Pros:** Easy to interpret, handles non-linear data  
**Cons:** Can overfit without pruning  
**Hyperparameters:** `max_depth=5`, `random_state=42`

---

### 3. Random Forest
**Type:** Ensemble (multiple trees)  
**Pros:** High accuracy, robust  
**Cons:** Slower training, less interpretable  
**Hyperparameters:** `n_estimators=100`, `max_depth=5`, `random_state=42`

---

### 4. K-Nearest Neighbors
**Type:** Instance-based  
**Pros:** Simple concept, no training  
**Cons:** Slow prediction, sensitive to scale  
**Hyperparameters:** `n_neighbors=5`

---

## ğŸ› ï¸ Technologies

- **Language:** Python 3.8+
- **Data Processing:** Pandas, NumPy
- **Machine Learning:** Scikit-learn
- **Model Persistence:** Joblib

## ğŸ“ Next Steps

### Immediate Tasks
- [ ] Evaluate models on test set
- [ ] Generate confusion matrices
- [ ] Calculate precision, recall, F1-score
- [ ] Compare model performance

### Future Enhancements
- [ ] Cross-validation for robust evaluation
- [ ] Hyperparameter tuning (GridSearchCV)
- [ ] Feature importance analysis
- [ ] ROC curves and AUC scores
- [ ] Model explainability (SHAP values)
- [ ] Jupyter notebook with visualizations
- [ ] Model deployment pipeline

## ğŸ”— Related Projects

**Previous:** [01-health-data-analyzer](../01-health-data-analyzer/) - Data cleaning and EDA  
**Next:** Model evaluation and deployment (planned)

## ğŸ¤ Contributing

This is a portfolio project demonstrating machine learning workflow.

## ğŸ“„ License

This project is open source and available for educational purposes.

---

**Status:** âœ… Complete - Models Trained  
**Last Updated:** January 2025  
**Next Milestone:** Test Set Evaluation